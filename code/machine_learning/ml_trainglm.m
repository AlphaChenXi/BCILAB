function model = ml_trainglm(varargin)
% Variational Bayesian estimation in a Generalized Linear Model.
% Model = ml_trainglm(Trials, Targets, Options...)
%
% This method allows to learn parameters of a generalized linear model (GLM) set up to predict the
% target variable in which the unknowns are estimated using Variational Bayesian (VB) inference
% (yielding the posterior mean and covariance). The model is quite flexible and allows to set up
% multiple priors with Gaussian or super-Gaussian (e.g., Laplace, Sech^2, Student-T) distributions,
% possibly linked to the unknowns via known linear transformations (such as Wavelet, finite
% differences, Fourier transform), and can perform both regression and classification. However, the
% specification of the model is quite complicated due to the flexibility, so advanced functionality
% is only usable by experts.
%
% In:
%   Trials       : training data, as in ml_train
%                  in addition, it may be specified as UxVxN 3d matrix,
%                  with UxV-formatted feature matrices per trial (N trials), or
%                  as {{U1xV1,U2xV2,...}, {U1xV1,U2xV2,...}
%
%   Targets      : target variable, as in ml_train
%
%   Lambdas : Noise variance parameter. If multiple values are given, the one with the best evidence
%             is chosen. A good default value is 1. (default: 2.^(-4:0.5:4))
%
%   Options  : optional name-value parameters to control the training details:
%              'ptype': problem type: 'classification' (default) or 'regression'
%
%              'shape': if trials is a NxF 2d matrix of vectorized matrices,
%                           this is the dimensions of the matrices (default: Fx1)
%                       if trials is specified as an UxVxN 3d matrix, shape defaults to
%                           [U,V] and trials are vectorized into the regular [N, U*V]
%                       if shape is specified with one of the values being NaN,
%                           that value is set so that prod(shape)==F==U*V
%
%              'scaling': pre-scaling of the data (see hlp_findscaling for options) (default: 'std')
%
%              'setupfcn': Setup Function. Receives trials, targets and the options structure, and 
%                          returns the variables [X,y,B,pot,tau,G]. If empty, performs ridge
%                          regression or logistic regression depending on Type parameter.
%
%               for additional parameters see infEngine.m in the glm-ie directory.
%
% Out:
%   Models   : a predictive model
%
% See also:
%   ml_predictglm, dli
%
%                                Christian Kothe, Swartz Center for Computational Neuroscience, UCSD
%                                2011-07-08

% definition of the possible distributions supported by glm-ie
distributions = @(argname) arg_subswitch({lower(argname),argname},{'none'},{ ...
    'none', {}, ...
    'Laplace', { ...
        arg({'B','LinearOperator'},'@(x)x',[],'Linear transform. The distribution applies to the linearly transformed weight vector. Either an expression that is evaluated in the workspace or a function handle.'), ...
        arg({'G','Groups'},[],[],'Grouping matrix. Sparse matrix whose columns are indicator vectors for the respective groups. If empty, the columns of B*u-t'), ...
        arg({'t','Shifts'},1,[],'Shifts for an affine transform. Allows for a shifted distribution.'), ...
        arg({'tau','Scales'},1,[],'Scales of the potential. Allows for a scaled distribution.'), ...        
    }, ...
    'Gaussian', { ...
        arg({'B','LinearOperator'},'@(x)x',[],'Linear transform. The distribution applies to the linearly transformed weight vector. Either an expression that is evaluated in the workspace or a function handle.'), ...
        arg({'G','Groups'},[],[],'Grouping matrix. Sparse matrix whose columns are indicator vectors for the respective groups. If empty, the identity matrix is assumed.'), ...
        arg({'t','Shifts'},1,[],'Shifts for an affine transform. Allows for a shifted distribution.'), ...
        arg({'tau','Scales'},1,[],'Scales of the potential. Allows for a scaled distribution.'), ...        
    }, ...
    'Logistic', { ...
        arg({'B','LinearOperator'},'@(x)x',[],'Linear transform. The distribution applies to the linearly transformed weight vector. Either an expression that is evaluated in the workspace or a function handle.'), ...
        arg({'G','Groups'},[],[],'Grouping matrix. Sparse matrix whose columns are indicator vectors for the respective groups. If empty, the identity matrix is assumed.'), ...
        arg({'t','Shifts'},1,[],'Shifts for an affine transform. Allows for a shifted distribution.'), ...
        arg({'tau','Scales'},1,[],'Scales of the potential. Allows for a scaled distribution.'), ...        
    }, ...
    'StudentT', { ...
        arg({'B','LinearOperator'},'@(x)x',[],'Linear transform. The distribution applies to the linearly transformed weight vector. Either an expression that is evaluated in the workspace or a function handle.'), ...
        arg({'G','Groups'},[],[],'Grouping matrix. Sparse matrix whose columns are indicator vectors for the respective groups. If empty, the identity matrix is assumed.'), ...
        arg({'t','Shifts'},1,[],'Shifts for an affine transform. Allows for a shifted distribution.'), ...
        arg({'tau','Scales'},1,[],'Scales of the potential. Allows for a scaled distribution.'), ...        
        arg({'nu','DegreesOfFreedom'},1,[1 Inf],'Degrees of freedom.'), ...        
    }, ...
    'Sech2', { ...
        arg({'B','LinearOperator'},'@(x)x',[],'Linear transform. The distribution applies to the linearly transformed weight vector. Either an expression that is evaluated in the workspace or a function handle.'), ...
        arg({'G','Groups'},[],[],'Grouping matrix. Sparse matrix whose columns are indicator vectors for the respective groups. If empty, the identity matrix is assumed.'), ...
        arg({'t','Shifts'},1,[],'Shifts for an affine transform. Allows for a shifted distribution.'), ...
        arg({'tau','Scales'},1,[],'Scales of the potential. Allows for a scaled distribution.'), ...        
    }, ...
    'ExpPow', { ...
        arg({'B','LinearOperator'},'@(x)x',[],'Linear transform. The distribution applies to the linearly transformed weight vector. Either an expression that is evaluated in the workspace or a function handle.'), ...
        arg({'G','Groups'},[],[],'Grouping matrix. Sparse matrix whose columns are indicator vectors for the respective groups. If empty, the identity matrix is assumed.'), ...
        arg({'t','Shifts'},1,[],'Shifts for an affine transform. Allows for a shifted distribution.'), ...
        arg({'tau','Scales'},1,[],'Scales of the potential. Allows for a scaled distribution.'), ...        
        arg({'alpha','ShapeExponent'},8,[0 Inf],'Shape parameter (exponent). This is the beta parameter of a type-1 generalized Gaussian distribution.'), ...        
    }}, 'Distribution type. Defines a parametric prior distribution applied to a linear transform of the data.');

args = arg_define([0 2],varargin, ...
    arg_norep('trials'), ...
    arg_norep('targets'), ...
    ... % problem setup
    arg({'lambdas','NoiseVariance','Lambdas'}, 2.^(-4:0.5:4), [0 Inf], 'Noise variance. If multiple values are given, the one with the best evidence is used. A good search range is 2.^(-4:0.5:4).','shape','row'), ...
    arg({'ptype','Type'}, 'classification', {'classification','regression'}, 'Type of problem to solve.'), ...
    arg_sub({'priors','Priors'},{},{ ...
        distributions('Term1'), ...
        distributions('Term2'), ...
        distributions('Term3'), ...
        distributions('Term4'), ...
        distributions('Term5'), ...
        distributions('Term6'), ...
        distributions('Term7')}, 'Definition of the prior terms. Any combination of terms is permitted.'), ...
    arg({'scaling','Scaling'}, 'std', {'none','center','std','minmax','whiten'}, 'Pre-scaling of the data. For the regulariation to work best, the features should either be naturally scaled well, or be artificially scaled.'), ...    
    ... %
    ... % inference engine parameters
    arg({'outerNiter','OuterIterations'},10,uint32([1 5 25 1000]),'Outer loop iterations.'),...
    arg({'innerMVM','InnerIterations'},50,uint32([1 10 150 1000]),'Inner-loop iterations. Number of matrix-vector multiplications and/or conjugate gradient steps to perform in inner loop.'),...
    arg_subswitch({'outerMethod','OuterMethod'},'Lanczos',{ ...
        'full',{}, ...    
        'Lanczos',{ ...
            arg({'MVM','LanczosVectors'},50,uint32([1 10 100 1000]),'Number of Lanczos vectors. This is the precision of the variational approximation. More more will yield a more complete posterior approximation.')},...
        'sample',{ ...
            arg({'NSamples','NumSamples'},10,uint32([1 5 100 1000]),'Number of Monte Carlo samples.'), ...
            arg({'Ncg','NumCGSteps'},20,uint32([1 5 100 1000]),'Number of Conjugate-Gradient steps.')}, ...
        'Woodbury',{}, ...
        'factorial',{}},'Posterior covariance algorithm. The ''full'' method produces an exact dense matrix, ''lanczos'' produces a low-rank approximation with a given number of Lanczos vectors, ''sample'' calculates a Monte Carlo estimate, ''woodbury'' calculates an exact solution using the Woodbury formula, requires that B=I, ''factorial'' yields a factorial (mean-field) estimate.'),...
    arg({'innerVBpls','InnerMethod'},'Conjugate Gradients',{ ...
        'Quasi-Newton',{ ...
            arg({'LBFGSnonneg','NonNegative'},false,[],'Non-negative. Whether to restrict the solution space to non-negative values.','guru',true)},...
        }, ... 
        'Truncated Newton',{ ...
            arg({'nit','NewtonSteps'},15,uint32([1 5 50 1000]),'Number of Newton steps.'),...
            arg({'nline','LineSearchSteps'},10,uint32([1 3 50 1000]),'Max line-search steps. Maximum number of bisections in Brent''s line-search algorithm.','guru',true),...
            arg({'exactNewt','ExactNewton'},false,[],'Compute exact direction. This is instead of CG; works only if B=1 and X is numeric.','guru',true),...
        },...
        'Backtracking Conjugate Gradients',{...
            arg({'nline','LineSearchSteps'},20,uint32([1 3 50 1000]),'Max line-search steps. Maximum number of backtracking steps in line search.','guru',true),...        
            arg({'al','ArmijoAlpha'},0.01,[0 0.001 1 Inf],'Armijo alpha parameters. Parameter in Armijo-rule line search.','guru',true),...        
            arg({'be','ArmijoShrink'},0.6,[0 0.1 0.9 1],'Armijo shrinking parameter. This is the beta or tau shrinking parameter in the line search.','guru',true),...        
            arg({'cmax','MaxStepsize'},1,[0 Inf],'Maximum stepsize. Scaling factor for the gradient direction.','guru',true),...        
            arg({'eps_grad','GradientTol'},1e-10,[0 Inf],'Gradient norm convergence threshold.','guru',true),...
            arg({'nrestart','Numrestarts'},0,uint32([0 0 10 100]),'Max CG restarts.','guru',true),...
        }, ...
        'Conjugate Gradients', {...
        }, ...
        'Split-Bregman',{...            
            arg({'SBeta','ConstraintCoefficient'},1,[],'Coefficient for constraint term. Data is rescaled such that 1 should work.','guru',true),...        
            arg({'SBga','RegularizationParameter'},[],[],'Regularization parameter. Best to leave unspecified; default is then 0.01/lambda.','guru',true),...        
            arg({'SBinner','InnerSteps'},30,uint32([1 5 100 1000]),'Number of (inner) loop steps. For the constraint. Sometimes 5-10 iterations are OK.','guru',true),...
            arg({'SBouter','OuterSteps'},15,uint32([1 5 100 1000]),'Number of (outer) Bregman iterations.','guru',true),...
            arg({'SBncg','CGSteps'},50,uint32([1 5 100 1000]),'Number of CG steps.','guru',true),...
        }, ...
        'Barzilai-Borwein',{...
            arg({'eps_grad','GradientTol'},1e-10,[0 Inf],'Gradient norm convergence threshold.','guru',true),...
            arg({'plsBBstepsizetype','EquationFive'},1,[],'Use equation 5 for update. Otherwise uses (equivalent) equation 6 from BB paper.','guru',true),...
        },'PLS solver. Penalized least-squares solver to use in the inner loop; Quasi-Newton (L-BFGS) is one of the most efficient method, but depends on C code; Truncated-Newton performs TN with CG-approximated Newton steps, Backtracking Conjugate Gradients uses CG with Armijo backtracking, Conjugate Gradients uses Carl Rasmussen''s minimize function using Polak-Ribiere line search, Split-Bregman uses an augmented Lagrangian / Bregman splitting approach, Barzilai-Borwein uses a stepsize-adjusted gradient method without descent guarantee.'), ...        
    arg({'outerZinit','InitialUpperBound'},0.05,[],'Initial upper bound.','guru',true),...
    arg({'outerGainit','InitialVariationalParam'},1,[],'Initial variational parameter.','guru',true),...    
    
    arg({'shape','Shape'}, [], [], 'Shape of the feature matrices. If given as [X,NaN] or [NaN,X], such that X is a divisor of the number of features F, the NaN is replaced by F/X.','shape','row'), ...
    arg({'vectorize_trials','VectorizeTrials'}, false, [], 'Vectorize trial matrices. Auto-determined if left unspecified.'), ...
    arg({'verbosity','Verbosity'},0,[0 2],'Verbosity level. Set to 0=disable output, 1=show outer-loop output, 2=show inner-loop outputs, too.'), ...

% TODO: 
% * map outerMetod args to outerVarOpts
% * map outerMethod.arg_selection to outerMethod
% * append contents of innerVBpls to opts
% * map innerVBpls.arg_selection to innerVBpls
% * rewrite innerVBpls based on rewrite rules
% * map verbosity to outerOutput and innerOutput
% * if SBga is empty, the field should be removed from the split-bregman solver variant
% * use lower() on the outerMethod

%
% implement plsLBFGS using liblbfgs instead :)

%    arg({'innerOutput','InnerVerbose'},false,[],'Verbose inner loop. Show diagonstic output in the inner loop of the algorithm.'),...
%    arg({'outerOutput','OuterVerbose','verbose'},false,[],'Verbose outer loop. Show diagonstic output in the outer loop of the algorithm.'),...


arg_toworkspace(args);

% set default setup function
if isempty(setupfcn)
    setupfcn = @default_setup; end

% get the correct feature matrix shape
if isempty(shape) %#ok<*NODEF>
    if ndims(trials) == 3
        shape = [size(trials,1) size(trials,2)];
        % ... also make sure that the trials are vectorized
        trials = double(reshape(trials,[],size(trials,3))');
        vectorize_trials = true;
    else
        shape = [size(trials,2) 1];
    end
elseif size(shape,1) == 1
    nf = size(trials,2);
    ni = isnan(shape);
    if any(ni)
        % if necessary, set NaN shape parameters appropriately
        shape(ni) = nf / shape(~ni);
    elseif nf ~= shape(1)*shape(2)
        % otherwise check for consistency
        error('shape parameter is inconsistent with feature space dimension.');
    end
end


% pre-process the data
if strcmp(ptype,'classification')
    classes = unique(targets);
    if length(classes) > 2
        % in the multi-class case we use the voter
        model = ml_trainvote(trials, targets, '1v1', @ml_trainglm, @ml_predictglm, varargin{:},'shape',shape,'vectorize_trials',vectorize_trials);
        return
    elseif length(classes) == 1
        error('BCILAB:only_one_class','Your training data set has no trials for one of your classes; you need at least two classes to train a classifier.\n\nThe most likely reasons are that one of your target markers does not occur in the data, or that all your trials of a particular class are concentrated in a single short segment of your data (10 or 20 percent). The latter would be a problem with the experiment design.');
    else       
        % optionally scale the data
        sc_info = hlp_findscaling(trials,scaling);
        trials = hlp_applyscaling(trials,sc_info);
        % remap target labels to -1,+1
        targets(targets==classes(1)) = -1;
        targets(targets==classes(2)) = +1;
    end
elseif strcmp(ptype,'regression')
    classes = [];
    % scale the data
    sc_info = hlp_findscaling(trials,scaling);
    trials = hlp_applyscaling(trials,sc_info);
else
    error('Unrecognized problem type.');
end

% rewrite arguments
args.innerVBpls = hlp_rewrite(args.innerVBpls,'Quasi-Newton','plsLBFGS','Truncated Newton','plsTN','Backtracking Conjugate Gradients','plsCGBT','Conjugate Gradients','plsCG','Split-Bregman','plsSB','Barzilai-Borwein','plsBB');

% call setup function
if nargout(setupfcn) == 5
    [X,y,B,pot,tau] = setupfcn(trials,targets,shape,args);
    G = [];
else
    [X,y,B,pot,tau,G] = setupfcn(trials,targets,shape,args);
end

if length(lambdas) > 1
    % empirically find lambda with best evidence
    for k=length(lambdas):-1:1
        [uinf,ga,b,z,nlZ(k),Q,T] = hlp_diskcache('predictivemodels',@dli,X,y,lambdas(k),B,pot,tau,rmfield(args,{'trials','targets','lambdas','ptype','scaling','setupfcn','shape','doinspect'}),G); end %#ok<NASGU,ASGLU>
    model.lambda_search = nlZ;
    model.lambda_best = lambdas(argmin(nlZ));
else
    model.lambda_best = lambdas;
end

% run inference
[uinf,ga,b,z,nlZ,Q,T] = hlp_diskcache('predictivemodels',@dli,X,y,model.lambda_best,B,pot,tau,rmfield(args,{'trials','targets','lambdas','ptype','scaling','setupfcn','shape','doinspect'}),G); %#ok<ASGLU>

% add misc meta-data to the model
model.ptype = ptype;
model.classes = classes;
model.sc_info = sc_info;
model.shape = shape;
model.w = uinf;
model.vectorize = vectorize_trials;


% note: I can implement general-purpose evidence maximization quite easily, just by running
% utl_gridsearch with all model parameters, including noise cov exposed :)

%  X   [mxn]  measurement matrix or operator
%  y   [mx1]  measurement vector
%  s2  [1x1]  measurement variance
%  B   [qxn]  matrix or operator
%  pot        potential function handle or function name string from pot/pot*.m
%  tau [qx1]  scale parameters of the potentials
%  t [qx1]    offset parameters of the potentials
%  G [dxn] is a "grouping matrix" --> acts as sqrt(G*x^2), that is, the nonzeros in each column of G define
%     a separate group; there can be overlapping groups, of course (but what that means is another
%     question)


function [X,y,B,pot,tau,G] = default_setup(trials,targets,shape,opts)
[n,f] = size(trials);
if strcmp(opts.ptype,'regression')
    % ridge regression
    X = trials;
    y = targets;
    B = eye(f);
    pot = @potGauss;
    tau = ones(f,1);
    G = [];
else
    % logistic regression
    X = eye(f);
    y = zeros(f,1);
    B = trials;
    pot = @potLogistic;
    tau = targets(:);
    G = [];
end



